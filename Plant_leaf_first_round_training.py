# -*- coding: utf-8 -*-
"""plant_2w.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DHEQvot-27y7ClDBHmdmR_OThSCQs4xD

### load data
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("emmarex/plantdisease")

print("Path to dataset files:", path)

import os

# Replace 'path' with the actual variable where your dataset path is stored
dataset_path = path  # This is the path variable from your kagglehub download

# List top-level files and folders
for root, dirs, files in os.walk(dataset_path):
    print(f"Directory: {root}")
    for name in dirs:
        print(f"Folder: {name}")
    for name in files:
        print(f"File: {name}")
    # Break after the first directory level to avoid too much output
    break

# Path to the PlantVillage folder
plant_village_path = os.path.join(dataset_path, "PlantVillage")

# List all folders inside PlantVillage
for root, dirs, files in os.walk(plant_village_path):
    print(f"Directory: {root}")
    for name in dirs:
        print(f"Folder: {name}")
    # Print a sample of files in each folder
    for name in files[:5]:  # Show first 5 files as a sample
        print(f"File: {name}")
    # Break after the first directory level to avoid too much output
    break

# Initialize lists to store image paths and labels
image_paths = []
labels = []

# Loop through each category folder in PlantVillage
for category_folder in os.listdir(plant_village_path):
    category_path = os.path.join(plant_village_path, category_folder)
    if os.path.isdir(category_path):  # Ensure it's a directory
        for image_file in os.listdir(category_path):
            if image_file.lower().endswith(('.jpg', '.jpeg', '.png')):  # Image file extensions
                image_paths.append(os.path.join(category_path, image_file))
                labels.append(category_folder)  # Folder name as label (y)

# Verify the data collection
print(f"Collected {len(image_paths)} images across {len(set(labels))} categories.")
print("Sample labels and paths:")
for i in range(10):  # Show 10 samples for verification
    print(f"Label: {labels[i]}, Path: {image_paths[i]}")

"""## folder name as category"""

# Mapping from original folder names to better, more readable category names
label_map = {
    "Tomato_Early_blight": "Tomato Early Blight",
    "Pepper__bell___Bacterial_spot": "Pepper Bell Bacterial Spot",
    "Pepper__bell___healthy": "Pepper Bell Healthy",
    "Potato___Early_blight": "Potato Early Blight",
    "Potato___healthy": "Potato Healthy",
    "Potato___Late_blight": "Potato Late Blight",
    "Tomato_Bacterial_spot": "Tomato Bacterial Spot",
    "Tomato_Leaf_Mold": "Tomato Leaf Mold",
    "Tomato_Septoria_leaf_spot": "Tomato Septoria Leaf Spot",
    "Tomato_Spider_mites_Two_spotted_spider_mite": "Tomato Spider Mites",
    "Tomato__Target_Spot": "Tomato Target Spot",
    "Tomato__Tomato_YellowLeaf__Curl_Virus": "Tomato Yellow Leaf Curl Virus",
    "Tomato__Tomato_mosaic_virus": "Tomato Mosaic Virus",
    "Tomato_healthy": "Tomato Healthy",  # Fixed mapping
    "Tomato_Late_blight": "Tomato Late Blight"
}

# Initialize lists to store image paths and mapped labels
image_paths = []
labels = []

# Loop through each category folder in PlantVillage and apply label mapping
for category_folder in os.listdir(plant_village_path):
    category_path = os.path.join(plant_village_path, category_folder)
    if os.path.isdir(category_path):  # Ensure it's a directory
        # Use mapped label or default to original folder name if not in label_map
        label = label_map.get(category_folder, category_folder)
        for image_file in os.listdir(category_path):
            if image_file.lower().endswith(('.jpg', '.jpeg', '.png')):  # Image file extensions
                image_paths.append(os.path.join(category_path, image_file))
                labels.append(label)  # Mapped label as y

# Verify the data collection with mapped labels
print(f"Collected {len(image_paths)} images across {len(set(labels))} categories.")
print("Sample labels and paths:")
for i in range(10):  # Show 10 samples for verification
    print(f"Label: {labels[i]}, Path: {image_paths[i]}")

"""## split training and testing dataset"""

from sklearn.model_selection import train_test_split

# Train-test split (80% train, 20% test)
train_paths, test_paths, train_labels, test_labels = train_test_split(
    image_paths, labels, test_size=0.2, stratify=labels, random_state=42
)

print(f"Training set: {len(train_paths)} images")
print(f"Testing set: {len(test_paths)} images")

"""## Data Loader"""

from torchvision import transforms
from torch.utils.data import DataLoader, Dataset
from PIL import Image

# Define transformations
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Dataset class
class PlantDataset(Dataset):
    def __init__(self, image_paths, labels, transform=None):
        self.image_paths = image_paths
        self.labels = labels
        self.transform = transform

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        image = Image.open(self.image_paths[idx]).convert("RGB")
        label = categories.index(self.labels[idx])  # Convert label to index
        if self.transform:
            image = self.transform(image)
        return image, label

# Create DataLoaders
train_dataset = PlantDataset(train_paths, train_labels, transform=transform)
test_dataset = PlantDataset(test_paths, test_labels, transform=transform)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

"""# Train the ViT model using the original ~20,000 images (without SAM)

## Train and Fine-Tune the Model
"""

from transformers import ViTForImageClassification
import torch
from torch.optim import AdamW
from torch.nn import CrossEntropyLoss
from transformers import get_scheduler
from tqdm import tqdm


categories = list(label_map.values())  # Extract the unique category names from label_map


# Load the pre-trained ViT model
model = ViTForImageClassification.from_pretrained(
    "google/vit-base-patch16-224-in21k",
    num_labels=len(categories),  # Number of categories
    id2label={i: label for i, label in enumerate(categories)},  # Index-to-label mapping
    label2id={label: i for i, label in enumerate(categories)}   # Label-to-index mapping
)

# Move the model to the GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Define optimizer, loss function, and scheduler
optimizer = AdamW(model.parameters(), lr=5e-5)
criterion = CrossEntropyLoss()
num_training_steps = len(train_loader) * 5
scheduler = get_scheduler("linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)

# Training loop
num_epochs = 4
model.train()

for epoch in range(num_epochs):
    print(f"Epoch {epoch + 1}/{num_epochs}")
    running_loss = 0.0
    correct = 0
    total = 0

    loop = tqdm(train_loader, leave=True)
    for images, labels in loop:
        images, labels = images.to(device), labels.to(device)

        # Forward pass
        outputs = model(images)
        loss = criterion(outputs.logits, labels)

        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        scheduler.step()

        # Update metrics
        running_loss += loss.item()
        _, predicted = torch.max(outputs.logits, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

        # Update progress bar
        loop.set_description(f"Epoch {epoch + 1}")
        loop.set_postfix(loss=loss.item(), accuracy=100 * correct / total)

    print(f"Epoch {epoch + 1} completed. Loss: {running_loss / len(train_loader):.4f}, Accuracy: {100 * correct / total:.2f}%")

"""## Save the trained model to google drive

"""

from transformers import ViTForImageClassification, ViTImageProcessor
import os

# Initialize the processor
processor = ViTImageProcessor.from_pretrained("google/vit-base-patch16-224-in21k")

# Create a directory to save the model and processor
save_directory = "./fine_tuned_vit"
os.makedirs(save_directory, exist_ok=True)

# Save the processor (used for preprocessing images)
processor.save_pretrained(save_directory)

# Save the fine-tuned model (includes weights and configuration)
model.save_pretrained(save_directory)

print(f"Model and processor saved in {save_directory}")

from google.colab import drive
import shutil

# Mount Google Drive
drive.mount('/content/drive')

# Copy the saved model to Google Drive
shutil.copytree('/content/fine_tuned_vit', '/content/drive/MyDrive/ViT-plant/fine_tuned_vit')
print("Model saved to Google Drive at /content/drive/MyDrive/ViT-plant/fine_tuned_vit")

"""# Load the Trained Vit model with original 20k images

"""

from google.colab import drive
from transformers import ViTForImageClassification, ViTImageProcessor
import torch

# Mount Google Drive
drive.mount('/content/drive')

# Define the path to the saved model on Google Drive
model_path = '/content/drive/MyDrive/ViT-plant/fine_tuned_vit'

# Load the processor and model
processor = ViTImageProcessor.from_pretrained(model_path)
model = ViTForImageClassification.from_pretrained(model_path)

print("Model and processor loaded successfully!")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

"""## Evaluate model with test Accuracy, Inference Time, Robustness"""

from transformers import ViTForImageClassification
import torch
from torch.optim import AdamW
from torch.nn import CrossEntropyLoss
from transformers import get_scheduler
from tqdm import tqdm
import time


categories = list(label_map.values())
# ðŸŸ¢ Test Accuracy Calculation
model.eval()  # Turn off dropout, batch norm, etc.
correct = 0  # Number of correct predictions
total = 0  # Total number of samples

# Disable gradient computation (for faster performance)
with torch.no_grad():
    for images, labels in test_loader:  # Loop through the test DataLoader
        # Move images and labels to the GPU (if available)
        images, labels = images.to(device), labels.to(device)

        # Forward pass through the model
        outputs = model(images)  # Handles both ResNet and ViT cases (ResNet: outputs, ViT: outputs.logits)

        # Get predictions (class with the maximum score)
        _, predicted = torch.max(outputs.logits if hasattr(outputs, 'logits') else outputs, 1)  # Handles ViT and ResNet

        # Count total samples and correct predictions
        total += labels.size(0)  # Total number of images in the batch
        correct += (predicted == labels).sum().item()  # Count the number of correct predictions

# Calculate accuracy as a percentage
test_accuracy = 100 * correct / total
print(f"Test Accuracy: {test_accuracy:.2f}%")


# ðŸŸ¢ Inference Time Calculation
model.eval()
inference_times = []

# Disable gradient computation (for faster performance)
with torch.no_grad():
    for images, labels in tqdm(test_loader, desc="Calculating Inference Time"):
        # Move images and labels to the GPU (if available)
        images, labels = images.to(device), labels.to(device)

        # Measure inference time
        start_time = time.time()
        outputs = model(images)  # Handles both ResNet and ViT cases (ResNet: outputs, ViT: outputs.logits)
        inference_time = time.time() - start_time
        inference_times.append(inference_time / images.size(0))  # Time per image

# Calculate average inference time
avg_inference_time = sum(inference_times) / len(inference_times)
print(f"Average Inference Time: {avg_inference_time * 1000:.2f} ms/image")


# ðŸŸ¢ Robustness Test Calculation
model.eval()
correct = 0  # Number of correct predictions
total = 0  # Total number of samples
noise_level = 0.1

# Disable gradient computation (for faster performance)
with torch.no_grad():
    for images, labels in tqdm(test_loader, desc="Testing Robustness"):
        # Add random noise to the images
        noise = torch.randn_like(images) * noise_level  # Create random noise
        noisy_images = torch.clamp(images + noise, 0, 1)  # Clamp pixel values to [0, 1]

        # Move images and labels to the GPU (if available)
        noisy_images, labels = noisy_images.to(device), labels.to(device)

        # Forward pass with noisy images
        outputs = model(noisy_images)  # Handles both ResNet and ViT cases (ResNet: outputs, ViT: outputs.logits)

        # Get predictions (class with the maximum score)
        _, predicted = torch.max(outputs.logits if hasattr(outputs, 'logits') else outputs, 1)  # Handles ViT and ResNet

        # Count total samples and correct predictions
        total += labels.size(0)  # Total number of images in the batch
        correct += (predicted == labels).sum().item()  # Count the number of correct predictions

# Calculate robustness accuracy as a percentage
robustness_accuracy = 100 * correct / total
print(f"Robustness Test Accuracy with {noise_level * 100}% noise: {robustness_accuracy:.2f}%")

"""# Train the Resnet model using the original ~20,000 images (without SAM)"""

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import models, transforms
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm

categories = list(label_map.values())  # Extract the unique category names from label_map
# ------------------------------------------
# ðŸŸ¢ Step 1: Load the Pre-trained ResNet Model
# ------------------------------------------
# Load ResNet50 (can also use resnet18, resnet34, resnet101, resnet152)
model = models.resnet50(pretrained=True)  # Load a ResNet50 model pre-trained on ImageNet

# Modify the final fully connected layer to match the number of classes in your dataset
num_features = model.fc.in_features  # Get the number of input features for the FC layer
model.fc = nn.Linear(num_features, len(categories))  # Change the output to match the number of classes

# Move model to the GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# ------------------------------------------
# ðŸŸ¢ Step 2: Define Loss, Optimizer, and Scheduler
# ------------------------------------------
optimizer = optim.AdamW(model.parameters(), lr=5e-5)  # Use AdamW optimizer (similar to ViT)
criterion = nn.CrossEntropyLoss()  # Loss function for multi-class classification
num_training_steps = len(train_loader) * 10  # Assuming 10 epochs
scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.0, total_iters=num_training_steps)

# ------------------------------------------
# ðŸŸ¢ Step 3: Training Loop
# ------------------------------------------
num_epochs = 7  # Number of epochs
model.train()  # Set the model to training mode

for epoch in range(num_epochs):
    print(f"Epoch {epoch + 1}/{num_epochs}")
    running_loss = 0.0
    correct = 0
    total = 0

    loop = tqdm(train_loader, leave=True)  # Training progress bar
    for images, labels in loop:
        images, labels = images.to(device), labels.to(device)

        # Forward pass
        outputs = model(images)  # Get model predictions
        loss = criterion(outputs, labels)  # Calculate the loss

        # Backward pass
        optimizer.zero_grad()  # Zero out previous gradients
        loss.backward()  # Backpropagate the loss
        optimizer.step()  # Update model weights
        scheduler.step()  # Update learning rate scheduler

        # Update metrics
        running_loss += loss.item()
        _, predicted = torch.max(outputs, 1)  # Get predicted class for each image
        total += labels.size(0)  # Count total images in the batch
        correct += (predicted == labels).sum().item()  # Count correct predictions

        # Update the progress bar with loss and accuracy
        loop.set_description(f"Epoch {epoch + 1}")
        loop.set_postfix(loss=loss.item(), accuracy=100 * correct / total)

    # Print epoch summary
    print(f"Epoch {epoch + 1} completed. Loss: {running_loss / len(train_loader):.4f}, Accuracy: {100 * correct / total:.2f}%")

from google.colab import drive
import shutil
import os
import torch

# Mount Google Drive
drive.mount('/content/drive')

# Define the path to save the model
drive_path = '/content/drive/MyDrive/ViT-plant/fine_tuned_resnet/'
os.makedirs(drive_path, exist_ok=True)

# Save model weights only
torch.save(model.state_dict(), os.path.join(drive_path, "resnet50_fine_tuned_weights.pth"))

# Save the entire model (architecture + weights)
torch.save(model, os.path.join(drive_path, "resnet50_fine_tuned_full.pth"))

print(f"ResNet model saved to Google Drive at {drive_path}")

"""## Evaluate model with test Accuracy, Inference Time, Robustness"""

import torch
import time
from tqdm import tqdm

# ðŸŸ¢ Test Accuracy Calculation
model.eval()  # Turn off dropout, batch norm, etc.
correct = 0  # Number of correct predictions
total = 0  # Total number of samples

# Disable gradient computation (for faster performance)
with torch.no_grad():
    for images, labels in test_loader:  # Loop through the test DataLoader
        # Move images and labels to the GPU (if available)
        images, labels = images.to(device), labels.to(device)

        # Forward pass through the ResNet model
        outputs = model(images)  # For ResNet, 'outputs' is the logits

        # Get predictions (class with the maximum score)
        _, predicted = torch.max(outputs, 1)  # Get the class with the highest score

        # Count total samples and correct predictions
        total += labels.size(0)  # Total number of images in the batch
        correct += (predicted == labels).sum().item()  # Count the number of correct predictions

# Calculate accuracy as a percentage
test_accuracy = 100 * correct / total
print(f"Test Accuracy: {test_accuracy:.2f}%")

# ðŸŸ¢ Inference Time Calculation
model.eval()
inference_times = []

# Disable gradient computation (for faster performance)
with torch.no_grad():
    for images, labels in tqdm(test_loader, desc="Calculating Inference Time"):
        # Move images and labels to the GPU (if available)
        images, labels = images.to(device), labels.to(device)

        # Measure inference time
        start_time = time.time()
        outputs = model(images)  # Forward pass
        inference_time = time.time() - start_time
        inference_times.append(inference_time / images.size(0))  # Time per image

# Calculate average inference time
avg_inference_time = sum(inference_times) / len(inference_times)
print(f"Average Inference Time: {avg_inference_time * 1000:.2f} ms/image")


# ðŸŸ¢ Robustness Test Calculation
model.eval()
correct = 0  # Number of correct predictions
total = 0  # Total number of samples
noise_level = 0.1  # Percentage of noise to add

# Disable gradient computation (for faster performance)
with torch.no_grad():
    for images, labels in tqdm(test_loader, desc="Testing Robustness"):
        # Add random noise to the images
        noise = torch.randn_like(images) * noise_level  # Create random noise
        noisy_images = torch.clamp(images + noise, 0, 1)  # Clamp pixel values to [0, 1]

        # Move images and labels to the GPU (if available)
        noisy_images, labels = noisy_images.to(device), labels.to(device)

        # Forward pass with noisy images
        outputs = model(noisy_images)  # For ResNet, 'outputs' is the logits

        # Get predictions (class with the maximum score)
        _, predicted = torch.max(outputs, 1)  # Get the class with the highest score

        # Count total samples and correct predictions
        total += labels.size(0)  # Total number of images in the batch
        correct += (predicted == labels).sum().item()  # Count the number of correct predictions

# Calculate robustness accuracy as a percentage
robustness_accuracy = 100 * correct / total
print(f"Robustness Test Accuracy with {noise_level * 100}% noise: {robustness_accuracy:.2f}%")























